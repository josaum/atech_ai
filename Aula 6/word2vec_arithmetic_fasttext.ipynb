{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1d578c",
   "metadata": {},
   "source": [
    "\n",
    "## Aritmética do Word2Vec com FastText\n",
    "\n",
    "Com o modelo pré-treinado carregado, podemos agora explorar a aritmética de palavras. A seguir, demonstraremos como realizar operações aritméticas em vetores de palavras e como elas podem revelar relações semânticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70159263",
   "metadata": {},
   "source": [
    "\n",
    "## Introdução ao Word2Vec e FastText\n",
    "\n",
    "Word2Vec é uma técnica de aprendizado de máquina que treina um modelo de redes neurais para aprender representações vetoriais de palavras, chamadas \"embeddings\", de grandes conjuntos de dados de texto. Esses embeddings capturam o contexto das palavras em relação às outras, de forma que palavras com significados semelhantes tenham representações vetoriais próximas no espaço.\n",
    "\n",
    "FastText é uma biblioteca para aprendizado de representações de texto e classificação de texto desenvolvida pelo Facebook. Difere do Word2Vec tradicional por representar cada palavra como um saco de n-gramas de caracteres, permitindo que o modelo capture informações subpalavrares e seja mais robusto para lidar com palavras desconhecidas ou raras.\n",
    "\n",
    "Neste notebook, exploraremos como realizar a aritmética de palavras usando embeddings de palavras, demonstrando como as relações semânticas entre palavras podem ser capturadas e manipuladas matematicamente com a ajuda da biblioteca FastText.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1f828",
   "metadata": {},
   "source": [
    "\n",
    "## Configuração do Ambiente\n",
    "\n",
    "Para começar, precisamos instalar e importar as bibliotecas necessárias. O código abaixo instala a biblioteca FastText e importa tudo o que precisaremos para este tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82786893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install fasttext\n",
    "#!pip install huggingface_hub\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from huggingface_hub import hf_hub_download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466243cb",
   "metadata": {},
   "source": [
    "\n",
    "## Carregando o Modelo Pré-treinado\n",
    "\n",
    "Em vez de preparar e treinar um modelo a partir de um conjunto de dados de texto, vamos carregar um modelo FastText pré-treinado para o português.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92843933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-pt-vectors\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea04fe2",
   "metadata": {},
   "source": [
    "\n",
    "## Aritmética do Word2Vec com FastText\n",
    "\n",
    "Uma vez que o modelo é treinado, podemos explorar a aritmética de palavras. A ideia básica por trás da aritmética de palavras é que as relações entre palavras são capturadas de tal forma que operações matemáticas como adição e subtração podem mostrar relações semânticas entre diferentes palavras. Por exemplo, a famosa equação \"rei - homem + mulher = rainha\" mostra essa relação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ba73db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7363070845603943 rainha\n",
      "0.6031950116157532 princesa\n",
      "0.5876360535621643 rainha-mãe\n",
      "0.5831598043441772 súdita\n",
      "0.582430362701416 rainha-consorte\n"
     ]
    }
   ],
   "source": [
    "# Substitua 'palavra1', 'palavra2', e 'palavra3' com palavras de sua escolha que formam uma relação analógica (por exemplo, 'rei', 'homem', 'mulher')\n",
    "word1, word2, word3 = 'rei', 'homem', 'mulher'\n",
    "\n",
    "# Buscar as palavras mais próximas na analogia\n",
    "nearest_words = model.get_analogies(word1, word2, word3, k=5)\n",
    "\n",
    "# Imprimir as palavras encontradas\n",
    "for word, similarity in nearest_words:\n",
    "    print(f\"{word} {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61120ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8040949702262878, 'chuvarada'),\n",
       " (0.7732902765274048, 'chuva.A'),\n",
       " (0.7683786749839783, 'chuvinha'),\n",
       " (0.7578243613243103, 'chuva.E'),\n",
       " (0.7484294176101685, 'chuva.É')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors(\"chuva\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d092f",
   "metadata": {},
   "source": [
    "\n",
    "## Similaridade de Cossenos\n",
    "\n",
    "A similaridade de cossenos é uma medida da similaridade entre dois vetores não-nulos em um espaço que possui um produto interno, que mede o cosseno do ângulo entre eles. A seguir, calcularemos a similaridade do cosseno entre diferentes pares de palavras para entender quão semanticamente próximas ou distantes elas estão.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0dc53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16110392"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def cosine_similarity(word1, word2):\n",
    "    return np.dot(model[word1], model[word2]) / (np.linalg.norm(model[word1]) * np.linalg.norm(model[word2]))\n",
    "\n",
    "# Substitua 'palavra1' e 'palavra2' com palavras de sua escolha\n",
    "word1, word2 = 'aviao', 'batata'\n",
    "similarity = cosine_similarity(word1, word2)\n",
    "\n",
    "similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7237d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56745315"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def cosine_similarity(word1, word2):\n",
    "    return np.dot(model[word1], model[word2]) / (np.linalg.norm(model[word1]) * np.linalg.norm(model[word2]))\n",
    "\n",
    "# Substitua 'palavra1' e 'palavra2' com palavras de sua escolha\n",
    "word1, word2 = 'Avião', 'Helicoptero'\n",
    "similarity = cosine_similarity(word1, word2)\n",
    "\n",
    "similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2369c",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusão\n",
    "\n",
    "Neste notebook, exploramos como usar um modelo FastText pré-treinado para realizar a aritmética do Word2Vec, calcular a similaridade do cosseno entre vetores de palavras e identificar idiomas em textos. Essas técnicas são poderosas para entender e manipular relações semânticas em grandes conjuntos de dados de texto e têm amplas aplicações em áreas como processamento de linguagem natural, sistemas de recomendação e muito mais.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atech-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
