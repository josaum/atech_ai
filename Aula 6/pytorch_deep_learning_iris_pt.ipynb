{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4bbe07",
   "metadata": {},
   "source": [
    "# PyTorch e Deep Learning: MLP Simples com o Conjunto de Dados Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de9f7e",
   "metadata": {},
   "source": [
    "\n",
    "Nesta segunda parte do notebook, continuaremos nossa jornada de aprendizado com o PyTorch e o Deep Learning. \n",
    "Vamos nos aprofundar nos fundamentos do Deep Learning e aplicar esse conhecimento construindo uma rede neural \n",
    "MLP (Perceptron Multicamadas) para classificar o conjunto de dados Iris.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694db968",
   "metadata": {},
   "source": [
    "\n",
    "## Fundamentos do Deep Learning\n",
    "\n",
    "### O que é Deep Learning?\n",
    "Deep Learning é um subcampo do aprendizado de máquina que lida com algoritmos inspirados na estrutura e \n",
    "função do cérebro chamados redes neurais artificiais. Justamente por serem inspiradas pelo cérebro humano, \n",
    "essas redes neurais são excepcionalmente boas em reconhecer padrões, que elas usam para classificar dados \n",
    "complexos como imagens, som, vídeo ou texto.\n",
    "\n",
    "### Redes Neurais e Perceptrons\n",
    "Uma rede neural é composta por camadas de nós, ou \"neurônios\", e as conexões entre eles. O bloco de construção \n",
    "mais simples de uma rede neural é o perceptron, que recebe várias entradas, aplica um conjunto de pesos a elas, \n",
    "soma-as e passa o resultado por uma função de ativação para produzir uma saída.\n",
    "\n",
    "### Funções de Ativação\n",
    "As funções de ativação ajudam a introduzir não-linearidades em nossa rede. Isso significa que, sem funções de \n",
    "ativação, a rede neural só seria capaz de aprender e realizar tarefas de maneira linear. Algumas funções de \n",
    "ativação comuns incluem ReLU, Sigmoid, e Tanh.\n",
    "\n",
    "### Backpropagation e Gradient Descent\n",
    "Backpropagation é o método usado para minimizar o erro de previsão da nossa rede, ajustando os pesos. Em conjunto \n",
    "com a técnica de otimização chamada Gradient Descent, podemos iterativamente ajustar os pesos e bias para minimizar \n",
    "o erro de saída.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atech-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
