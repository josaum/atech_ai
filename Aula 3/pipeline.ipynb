{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprenderemos a trabalhar *corretamente* com pipelines no scikit-learn.\n",
    "\n",
    "É importante entender que o conceito de pipelines é válido para qualquer conjunto de biblitecas, linguagens e ferramentas.\n",
    "\n",
    "Todo projeto precisa de um Pipeline de dados bem definido, bem documento e bem testado. Nesta sessão, abordaremos apenas o pipeline de transformação de dados para alimentar nossos modelos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Carregando os dados\n",
    "# https://archive.ics.uci.edu/dataset/19/car+evaluation\n",
    "column_names = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\n",
    "data = pd.read_csv(\"../dados/carros_usados/car.data\", header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicionário de dados dataset\n",
    "\n",
    "| Attribute Name | Role    | Type        | Demographic | Description                                | Units | Missing Values |\n",
    "|----------------|---------|-------------|-------------|--------------------------------------------|-------|----------------|\n",
    "| buying         | Feature | Categorical |             | buying price                               |       | false          |\n",
    "| maint          | Feature | Categorical |             | price of the maintenance                   |       | false          |\n",
    "| doors          | Feature | Categorical |             | number of doors                            |       | false          |\n",
    "| persons        | Feature | Categorical |             | capacity in terms of persons to carry      |       | false          |\n",
    "| lug_boot       | Feature | Categorical |             | the size of luggage boot                   |       | false          |\n",
    "| safety         | Feature | Categorical |             | estimated safety of the car                |       | false          |\n",
    "| class          | Target  | Categorical |             | evaluation level (unacceptable, acceptable, good, very good) | | false |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos doors e persons como variaveis numericas neste exemplo\n",
    "# Convertendo \"5-more\" e \"more\" para 5\n",
    "data['doors'] = data['doors'].replace('5more', 5).astype(int)\n",
    "data['persons'] = data['persons'].replace('more', 5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do conjunto de Treino e Validação 80% e Teste (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os dados em conjuntos de treinamento (90%) e teste (10%)\n",
    "# Definindo X e y\n",
    "X = data.drop(columns=[\"class\"])\n",
    "y = data[\"class\"]\n",
    "X_temp, X_test_final, y_temp, y_test_final = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do conjunto de Treino 80% e Validação 20%, já dentro dos 80% dos dados que não são de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo o conjunto de treinamento em subconjuntos de treinamento e validação para RandomizedSearchCV\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.8, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição dos pre-processamentos necessários para dados numéricos e categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo colunas numéricas e categóricas\n",
    "numeric_features = ['doors', 'persons']\n",
    "categorical_features = ['buying', 'maint', 'lug_boot', 'safety']\n",
    "\n",
    "# Criando o ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OrdinalEncoder(), categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())  # Inicial com LogisticRegression\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição do Espaço de Busca de modelos e seus hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=42)\n",
    "# Hiperparâmetros e modelos\n",
    "param_distributions = {\n",
    "    'LogisticRegression': {\n",
    "        'model__C': uniform(0.001, 10),\n",
    "        'model__penalty': ['l1', 'l2'],\n",
    "        'model__solver': ['liblinear']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': randint(50, 200),\n",
    "        'model__max_depth': randint(2, 20),\n",
    "        'model__min_samples_split': uniform(0.01, 0.2),\n",
    "        'model__min_samples_leaf': uniform(0.01, 0.2)\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model__n_estimators': randint(50, 200),\n",
    "        'model__learning_rate': uniform(0.01, 0.3),\n",
    "        'model__max_depth': randint(2, 10),\n",
    "        'model__min_samples_split': uniform(0.01, 0.2),\n",
    "        'model__min_samples_leaf': uniform(0.01, 0.2),\n",
    "        'model__subsample': uniform(0.5, 0.5)\n",
    "    }\n",
    "}\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da validação cruzada randomizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=3.746401188473625, model__penalty=l1, model__solver=liblinear; total time=   0.0s\n",
      "[CV] END model__C=3.746401188473625, model__penalty=l1, model__solver=liblinear; total time=   0.0s\n",
      "[CV] END model__C=5.987584841970366, model__penalty=l1, model__solver=liblinear; total time=   0.0s\n",
      "[CV] END model__C=1.8353478986616378, model__penalty=l2, model__solver=liblinear; total time=   0.0s\n",
      "[CV] END model__C=1.8353478986616378, model__penalty=l2, model__solver=liblinear; total time=   0.0s\n",
      "[CV] END model__C=3.746401188473625, model__penalty=l1, model__solver=liblinear; total time=   0.0s\n",
      "[CV] END model__C=5.987584841970366, model__penalty=l1, model__solver=liblinear; total time=   0.0s\n",
      "[CV] END model__C=1.8353478986616378, model__penalty=l2, model__solver=liblinear; total time=   0.0s\n",
      "[CV] END model__C=5.987584841970366, model__penalty=l1, model__solver=liblinear; total time=   0.0s\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END model__max_depth=5, model__min_samples_leaf=0.038573363584388155, model__min_samples_split=0.14017769458977059, model__n_estimators=102; total time=   0.1s\n",
      "[CV] END model__max_depth=8, model__min_samples_leaf=0.1693085973720466, model__min_samples_split=0.04668695797323276, model__n_estimators=121; total time=   0.1s\n",
      "[CV] END model__max_depth=8, model__min_samples_leaf=0.1693085973720466, model__min_samples_split=0.04668695797323276, model__n_estimators=121; total time=   0.2s\n",
      "[CV] END model__max_depth=8, model__min_samples_leaf=0.1693085973720466, model__min_samples_split=0.04668695797323276, model__n_estimators=121; total time=   0.2s\n",
      "[CV] END model__max_depth=8, model__min_samples_leaf=0.09916655057071823, model__min_samples_split=0.02999498316360058, model__n_estimators=137; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_leaf=0.038573363584388155, model__min_samples_split=0.14017769458977059, model__n_estimators=102; total time=   0.2s\n",
      "[CV] END model__max_depth=8, model__min_samples_leaf=0.09916655057071823, model__min_samples_split=0.02999498316360058, model__n_estimators=137; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_leaf=0.038573363584388155, model__min_samples_split=0.14017769458977059, model__n_estimators=102; total time=   0.1s\n",
      "[CV] END model__max_depth=8, model__min_samples_leaf=0.09916655057071823, model__min_samples_split=0.02999498316360058, model__n_estimators=137; total time=   0.2s\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END model__learning_rate=0.12236203565420874, model__max_depth=6, model__min_samples_leaf=0.04668695797323276, model__min_samples_split=0.16593820005455387, model__n_estimators=70, model__subsample=0.5780093202212182; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.12236203565420874, model__max_depth=6, model__min_samples_leaf=0.04668695797323276, model__min_samples_split=0.16593820005455387, model__n_estimators=70, model__subsample=0.5780093202212182; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.12236203565420874, model__max_depth=6, model__min_samples_leaf=0.04668695797323276, model__min_samples_split=0.16593820005455387, model__n_estimators=70, model__subsample=0.5780093202212182; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.016175348288740735, model__max_depth=3, model__min_samples_leaf=0.15439975445336496, model__min_samples_split=0.19771054180315006, model__n_estimators=179, model__subsample=0.5909124836035503; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.016175348288740735, model__max_depth=3, model__min_samples_leaf=0.15439975445336496, model__min_samples_split=0.19771054180315006, model__n_estimators=179, model__subsample=0.5909124836035503; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.05679835610086079, model__max_depth=4, model__min_samples_leaf=0.10184977839317343, model__min_samples_split=0.07674172222780437, model__n_estimators=153, model__subsample=0.8540362888980227; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.05679835610086079, model__max_depth=4, model__min_samples_leaf=0.10184977839317343, model__min_samples_split=0.07674172222780437, model__n_estimators=153, model__subsample=0.8540362888980227; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.05679835610086079, model__max_depth=4, model__min_samples_leaf=0.10184977839317343, model__min_samples_split=0.07674172222780437, model__n_estimators=153, model__subsample=0.8540362888980227; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.016175348288740735, model__max_depth=3, model__min_samples_leaf=0.15439975445336496, model__min_samples_split=0.19771054180315006, model__n_estimators=179, model__subsample=0.5909124836035503; total time=   0.4s\n",
      "Melhor Score: 0.8824593128390597\n",
      "Melhor Score: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
      "                                                  ['doors', 'persons']),\n",
      "                                                 ('cat', OrdinalEncoder(),\n",
      "                                                  ['buying', 'maint',\n",
      "                                                   'lug_boot', 'safety'])])),\n",
      "                ('model',\n",
      "                 GradientBoostingClassifier(learning_rate=0.05679835610086079,\n",
      "                                            max_depth=4,\n",
      "                                            min_samples_leaf=0.10184977839317343,\n",
      "                                            min_samples_split=0.07674172222780437,\n",
      "                                            n_estimators=153, random_state=42,\n",
      "                                            subsample=0.8540362888980227))])\n"
     ]
    }
   ],
   "source": [
    "# RandomizedSearchCV para cada modelo\n",
    "best_model = None\n",
    "best_score = 0\n",
    "for model_name, model in models.items():\n",
    "    pipeline.set_params(model=model)\n",
    "    search = RandomizedSearchCV(pipeline, param_distributions=param_distributions[model_name], \n",
    "                                n_iter=3, cv=3, n_jobs=-1, random_state=42, verbose=2)\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Selecionando o melhor modelo baseado na validação\n",
    "    score = accuracy_score(y_val, search.best_estimator_.predict(X_val))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model = search.best_estimator_\n",
    "\n",
    "    \n",
    "print(f\"Melhor Score: {best_score}\")\n",
    "print(f\"Melhor Score: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação final do modelo no conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor modelo: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
      "                                                  ['doors', 'persons']),\n",
      "                                                 ('cat', OrdinalEncoder(),\n",
      "                                                  ['buying', 'maint',\n",
      "                                                   'lug_boot', 'safety'])])),\n",
      "                ('model',\n",
      "                 GradientBoostingClassifier(learning_rate=0.05679835610086079,\n",
      "                                            max_depth=4,\n",
      "                                            min_samples_leaf=0.10184977839317343,\n",
      "                                            min_samples_split=0.07674172222780437,\n",
      "                                            n_estimators=153, random_state=42,\n",
      "                                            subsample=0.8540362888980227))])\n",
      "Acurácia no conjunto de teste: 0.9104046242774566\n"
     ]
    }
   ],
   "source": [
    "# Avaliando a performance do melhor modelo no conjunto de teste\n",
    "test_score = accuracy_score(y_test_final, best_model.predict(X_test_final))\n",
    "print(\"Melhor modelo:\", best_model)\n",
    "print(\"Acurácia no conjunto de teste:\", test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atech-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
