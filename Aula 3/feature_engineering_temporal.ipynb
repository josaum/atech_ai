{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldname, drop=True, time=False):\n",
    "    \"\"\"\n",
    "    Adiciona colunas ao dataframe relacionadas a uma coluna de data especificada.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - df (pd.DataFrame): DataFrame de entrada.\n",
    "    - fldname (str): Nome da coluna de data no DataFrame.\n",
    "    - drop (bool): Se verdadeiro, a coluna de data original será removida.\n",
    "    - time (bool): Se verdadeiro, colunas relacionadas ao tempo (hora, minuto, segundo) serão adicionadas.\n",
    "    \n",
    "    Retorna:\n",
    "    - None: O DataFrame original (df) é modificado no local.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Garante que a coluna especificada é do tipo datetime\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    \n",
    "    # Prefixo para os novos nomes de colunas\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    \n",
    "    # Atributos que serão extraídos da data\n",
    "    attr = ['Year', 'Month', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    \n",
    "    # Se o parâmetro time for verdadeiro, adiciona colunas relacionadas ao tempo\n",
    "    if time: \n",
    "        attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    \n",
    "    # Loop através dos atributos e adicione-os ao dataframe\n",
    "    for n in attr: \n",
    "        if n == 'Week': \n",
    "            # 'Week' foi depreciado, usando isocalendar().week em vez disso\n",
    "            df[targ_pre + n] = fld.dt.isocalendar().week\n",
    "        else:\n",
    "            df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    \n",
    "    # Adiciona uma coluna com o tempo em formato Unix\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    \n",
    "    # Remove a coluna de data original, se solicitado\n",
    "    if drop: \n",
    "        df.drop(fldname, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dados fictícios\n",
    "# Definindo as datas\n",
    "date_range = pd.date_range(start='2019-01-01', end='2020-12-31', freq='D')\n",
    "\n",
    "# Gerando características numéricas\n",
    "np.random.seed(42)  # Para reprodutibilidade\n",
    "temperatura = np.random.normal(25, 5, len(date_range))  # Média de 25 graus e desvio padrão de 5\n",
    "precipitacao = np.random.gamma(2, 10, len(date_range))  # Gamma distribuição para simular precipitação\n",
    "\n",
    "# Gerando características categóricas\n",
    "estacoes = ['primavera', 'verão', 'outono', 'inverno']\n",
    "evento_climatico = ['ensolarado', 'chuvoso', 'nublado']\n",
    "\n",
    "# Associando as estações aos meses\n",
    "estacao = np.array(estacoes * (len(date_range) // 4 + 1))[:len(date_range)]\n",
    "np.random.shuffle(estacao)  # Misturando as estações aleatoriamente\n",
    "evento = np.random.choice(evento_climatico, len(date_range))\n",
    "\n",
    "# Gerando a variável alvo fictícia\n",
    "# Suponha que o consumo de energia é maior em dias frios e em dias chuvosos\n",
    "consumo_energia = 1000 + (25 - temperatura) * 10 + precipitacao * 2 + np.random.normal(0, 50, len(date_range))\n",
    "\n",
    "# Criando o DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'data': date_range,\n",
    "    'temperatura': temperatura,\n",
    "    'precipitacao': precipitacao,\n",
    "    'estacao': estacao,\n",
    "    'evento_climatico': evento,\n",
    "    'consumo_energia': consumo_energia\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X e y\n",
    "y = df['consumo_energia']\n",
    "df = df.drop(columns='consumo_energia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering temporal\n",
    "add_datepart(df, 'data', drop=False, time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo transformadores\n",
    "\n",
    "# Identificando automaticamente características numéricas e categóricas\n",
    "numeric_features = df.select_dtypes(include=['int32','int64', 'float64']).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Definindo os pipelines conforme fornecido\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Transformador de colunas: aplica as transformações acima nas respectivas colunas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina o pré-processamento com o modelo em um único pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('modelo', RandomForestRegressor())\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CERTIFIQUE-SE QUE O DATAFRAME ESTEJA ORDENADO CRESCENTE PELA COLUNA DE DATA #\n",
    "## Identifique os índices únicos para cada data\n",
    "unique_dates = df['data'].drop_duplicates().sort_values()\n",
    "date_indices = range(len(unique_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a estratégia de validação cruzada respeitando a estrutura temporal\n",
    "tscv = TimeSeriesSplit(n_splits=5) # aqui será respeitada a estrutura temporal - dados em teste serão sempre em momento futuro aos dados do treino\n",
    "\n",
    "# Fazemos nossa função na mão para resolver o problema com temporalidade\n",
    "# Listas para armazenar scores de cada fold\n",
    "scores = []\n",
    "for train_date_idx, test_date_idx in tscv.split(date_indices):\n",
    "    # Converta índices de data em índices de dataframe\n",
    "    train_idx = df[df['data'].isin(unique_dates.iloc[train_date_idx])].index\n",
    "    test_idx = df[df['data'].isin(unique_dates.iloc[test_date_idx])].index\n",
    "    \n",
    "    # Separe os conjuntos de treinamento e teste usando os índices\n",
    "    X_train, y_train = df.drop(['data'], axis=1).iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = df.drop(['data'], axis=1).iloc[test_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Treine o pipeline no conjunto de treinamento\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Faça previsões no conjunto de teste\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calcule a métrica de erro (por exemplo, MAE)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Adicione o score à lista de scores\n",
    "    scores.append(mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule a média e o desvio padrão dos escores\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "print(f\"Média de MAE: {mean_score}, Desvio padrão: {std_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atech-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
