{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Introdução ao Aprendizado Não-Supervisionado\n",
    "\n",
    "O aprendizado não-supervisionado é uma categoria de técnicas de aprendizado de máquina em que os modelos são treinados usando um conjunto de dados que não tem rótulos anotados. O objetivo dessas técnicas é encontrar estruturas ocultas nos dados. Alguns dos usos comuns do aprendizado não-supervisionado incluem:\n",
    "\n",
    "- **Clusterização (ou agrupamento)**: Identificar grupos ou clusters de exemplos similares nos dados.\n",
    "- **Redução de dimensionalidade**: Simplificar os dados sem perder muita informação.\n",
    "  \n",
    "O aprendizado não-supervisionado contrasta com o aprendizado supervisionado, onde os modelos são treinados usando um conjunto de dados que tem rótulos anotados.\n",
    "\n",
    "---\n",
    "\n",
    "## Técnicas de Clusterização e Agrupamento\n",
    "\n",
    "### K-means\n",
    "\n",
    "O **K-means** é um dos algoritmos mais populares para clusterização. Ele particiona os dados em \\( k \\) clusters, onde \\( k \\) é pré-especificado. O algoritmo funciona iterativamente para atribuir cada ponto de dado a um dos \\( k \\) clusters com base nas características.\n",
    "\n",
    "**Vantagens**:\n",
    "- Rápido e eficiente em termos de custo computacional.\n",
    "- Fácil de entender e implementar.\n",
    "\n",
    "**Desvantagens**:\n",
    "- O número de clusters (\\( k \\)) precisa ser especificado antecipadamente.\n",
    "- Sensível à inicialização dos centróides.\n",
    "- Pode ficar preso em mínimos locais.\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "O **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** é um algoritmo baseado em densidade, o que significa que ele forma clusters a partir de regiões de alta densidade de pontos.\n",
    "\n",
    "**Vantagens**:\n",
    "- Pode encontrar clusters de formas arbitrárias.\n",
    "- Não precisa especificar o número de clusters.\n",
    "- Pode lidar com ruído.\n",
    "\n",
    "**Desvantagens**:\n",
    "- Não lida bem com clusters de diferentes densidades.\n",
    "- Sensível aos parâmetros.\n",
    "\n",
    "### HDBSCAN\n",
    "\n",
    "**HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)** é uma extensão do DBSCAN que pode detectar clusters de várias densidades.\n",
    "\n",
    "**Vantagens**:\n",
    "- Pode encontrar clusters de diversas formas e densidades.\n",
    "- Não precisa especificar o número de clusters.\n",
    "  \n",
    "**Desvantagens**:\n",
    "- Mais computacionalmente intenso do que DBSCAN.\n",
    "- Ainda sensível aos parâmetros.\n",
    "\n",
    "### Método do Cotovelo (Elbow Method)\n",
    "\n",
    "É uma técnica para encontrar o número ótimo de clusters para o K-means. O método envolve a execução do algoritmo K-means em um conjunto de dados para um intervalo de valores de \\( k \\) e, em seguida, para cada valor de \\( k \\), calcular a soma dos erros quadrados (SSE). À medida que \\( k \\) aumenta, a melhoria da SSE diminuirá e o valor de \\( k \\) onde essa mudança se torna insignificante é chamado de \"cotovelo\".\n",
    "\n",
    "---\n",
    "\n",
    "## Definição de Similaridade\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "É a distância \"em linha reta\" entre dois pontos no espaço euclidiano. É dada pela fórmula:\n",
    "\n",
    "$$\n",
    "\\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "![euclidian distance](euclidian_distance.png)\n",
    "\n",
    "### Manhattan Distance\n",
    "\n",
    "Também conhecida como distância \"city block\". É a distância entre dois pontos em uma grade (como o sistema de ruas de Manhattan) calculada somando as diferenças absolutas de suas coordenadas.\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} |p_i - q_i|\n",
    "$$\n",
    "\n",
    "![manhattan distance](manhattan_distance.png)\n",
    "\n",
    "### Minkowski Distance\n",
    "\n",
    "É uma generalização das distâncias euclidiana e manhattan. É dada pela fórmula:\n",
    "\n",
    "$$\n",
    "\\left( \\sum_{i=1}^{n} |p_i - q_i|^p \\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "Quando \\( p = 2 \\), é a distância euclidiana. Quando \\( p = 1 \\), é a distância manhattan.\n",
    "\n",
    "### Hamming Distance\n",
    "\n",
    "É utilizada para comparar strings de igual comprimento. Ela mede o número mínimo de substituições necessárias para mudar uma string para a outra ou o número mínimo de erros que poderiam ter transformado uma string na outra.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Distância Cosseno\n",
    "\n",
    "A distância cosseno mede a semelhança entre dois vetores com base no ângulo entre eles. Essa medida é especialmente útil em contextos onde a magnitude dos vetores não é tão relevante, mas a orientação deles é. Por exemplo, ela é frequentemente usada em análise de texto para comparar a semelhança entre documentos representados como vetores TF-IDF.\n",
    "\n",
    "O cosseno da semelhança entre dois vetores $ ( A ) e ( B ) $ é calculado usando a seguinte fórmula:\n",
    "\n",
    "$$\n",
    "\\text{similarity}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $ ( A \\cdot B ) $ é o produto escalar dos vetores $ ( A ) e ( B ) $.\n",
    "- $ ( |A| ) e ( |B| ) $ são as magnitudes (ou normas) dos vetores $ ( A ) e ( B ) $, respectivamente.\n",
    "\n",
    "A distância cosseno é então definida como:\n",
    "\n",
    "$$\n",
    "\\text{distância}(A, B) = 1 - \\text{similarity}(A, B)\n",
    "$$\n",
    "\n",
    "Assim, a distância cosseno varia entre 0 (indicando que os vetores são idênticos) e 2 (indicando que os vetores são diametralmente opostos). No entanto, em muitos contextos, os vetores são não-negativos (como em vetores TF-IDF), e a distância cosseno varia entre 0 e 1.\n",
    "\n",
    "**Vantagens**:\n",
    "- É menos sensível à magnitude dos vetores e se concentra mais na orientação, o que é útil em muitos contextos, como análise de texto.\n",
    "  \n",
    "**Desvantagens**:\n",
    "- Não é uma métrica de distância \"verdadeira\" no sentido matemático, pois pode violar a desigualdade triangular.\n",
    "\n",
    "![cosine distance](cosine_distance.png)\n",
    "\n",
    "### Produto Escalar\n",
    "\n",
    "O produto escalar, também conhecido como produto interno, é uma operação entre dois vetores que retorna um único valor numérico. Em contextos de aprendizado de máquina e processamento de linguagem natural, o produto escalar é frequentemente usado para medir a semelhança entre vetores.\n",
    "\n",
    "Dado dois vetores $ ( A ) e ( B ) $ de igual dimensão, o produto escalar é definido como:\n",
    "\n",
    "$$\n",
    "A \\cdot B = \\sum_{i=1}^{n} A_i \\times B_i\n",
    "$$\n",
    "\n",
    "Onde $ ( n ) $ é a dimensão dos vetores.\n",
    "\n",
    "Quando dois vetores são normalizados (ou seja, têm comprimento 1), o produto escalar indica o cosseno do ângulo entre eles. Um valor de produto escalar próximo de 1 indica que os vetores são muito similares, um valor próximo de -1 indica que são muito diferentes, e um valor próximo de 0 indica que são ortogonais (não relacionados).\n",
    "\n",
    "**Vantagens**:\n",
    "- Rápido de calcular.\n",
    "- Fornece uma medida direta da semelhança em termos de orientação dos vetores.\n",
    "\n",
    "**Desvantagens**:\n",
    "- Dependente da magnitude dos vetores. Dois vetores podem ter um produto escalar alto simplesmente porque um ou ambos têm magnitudes grandes, mesmo que não sejam semelhantes em termos de direção.\n",
    "- Não é uma métrica de distância no sentido tradicional.\n",
    "\n",
    "Para transformar o produto escalar em uma métrica de distância, uma abordagem comum é subtrair o produto escalar de um valor constante (por exemplo, o produto escalar máximo possível entre quaisquer dois vetores no conjunto de dados).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
